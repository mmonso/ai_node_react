API Gemini Developer

JavaScript

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "YOUR_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

await main();

####################################################################

Guia de início rápido da API Gemini

Neste guia de início rápido, mostramos como instalar o SDK escolhido e fazer sua primeira solicitação da API Gemini.

JavaScript

Instalar a biblioteca da API Gemini
Observação: estamos lançando um novo conjunto de bibliotecas da API Gemini, o SDK da IA generativa do Google.
Usando o Node.js v18 ou mais recente, instale o SDK da IA generativa do Google para TypeScript e JavaScript usando o seguinte comando npm:


npm install @google/genai

Faça sua primeira solicitação

Use o método generateContent para enviar uma solicitação à API Gemini.


import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "YOUR_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();

####################################################################


Compatibilidade com o OpenAI

Os modelos do Gemini podem ser acessados usando as bibliotecas OpenAI (Python e TypeScript / Javascript) com a API REST, atualizando três linhas de código e usando sua chave da API Gemini. Se você ainda não usa as bibliotecas OpenAI, recomendamos chamar a API Gemini diretamente.

Python
JavaScript
REST

import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Explain to me how AI works",
        },
    ],
});

console.log(response.choices[0].message);
O que mudou? Apenas três linhas.

api_key="GEMINI_API_KEY": substitua "GEMINI_API_KEY" pela chave de API Gemini, que você pode encontrar no Google AI Studio.

base_url="https://generativelanguage.googleapis.com/v1beta/openai/":instrui a biblioteca OpenAI a enviar solicitações para o endpoint da API Gemini em vez do endpoint padrão da OpenAI.

model="gemini-2.0-flash": estamos especificando o modelo gemini-2.0-flash, que é poderoso e eficiente.

Listar modelos
Confira uma lista de modelos do Gemini disponíveis:

Python
JavaScript
REST

import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const list = await openai.models.list();

  for await (const model of list) {
    console.log(model);
  }
}
main();
Recuperar um modelo
Extraia um modelo do Gemini:

Python
JavaScript
REST

import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const model = await openai.models.retrieve("gemini-2.0-flash");
  console.log(model.id);
}

main();
Streaming
A API Gemini oferece suporte a respostas de streaming.

Python
JavaScript
REST

import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ],
    stream: true,
  });

  for await (const chunk of completion) {
    console.log(chunk.choices[0].delta.content);
  }
}

main();
Chamadas de função
A chamada de função facilita a geração de saídas de dados estruturados de modelos generativos e é compatível com a API Gemini.

Python
JavaScript
REST

import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const messages = [{"role": "user", "content": "What's the weather like in Chicago today?"}];
  const tools = [
      {
        "type": "function",
        "function": {
          "name": "get_weather",
          "description": "Get the weather in a given location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. Chicago, IL",
              },
              "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
            },
            "required": ["location"],
          },
        }
      }
  ];

  const response = await openai.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: messages,
    tools: tools,
    tool_choice: "auto",
  });

  console.log(response);
}

main();
Compreensão de imagens
Os modelos Gemini são multimodais por natureza e oferecem a melhor performance da categoria em muitas tarefas de visão comuns.

Python
JavaScript
REST

import OpenAI from "openai";
import fs from 'fs/promises';

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function encodeImage(imagePath) {
  try {
    const imageBuffer = await fs.readFile(imagePath);
    return imageBuffer.toString('base64');
  } catch (error) {
    console.error("Error encoding image:", error);
    return null;
  }
}

async function main() {
  const imagePath = "Path/to/agi/image.jpeg";
  const base64Image = await encodeImage(imagePath);

  const messages = [
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "What is in this image?",
        },
        {
          "type": "image_url",
          "image_url": {
            "url": `data:image/jpeg;base64,${base64Image}`
          },
        },
      ],
    }
  ];

  try {
    const response = await openai.chat.completions.create({
      model: "gemini-2.0-flash",
      messages: messages,
    });

    console.log(response.choices[0]);
  } catch (error) {
    console.error("Error calling Gemini API:", error);
  }
}

main();
Gerar uma imagem
Observação: a geração de imagens está disponível apenas no nível pago.
Gerar uma imagem:

Python
JavaScript
REST

import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

async function main() {
  const image = await openai.images.generate(
    {
      model: "imagen-3.0-generate-002",
      prompt: "a portrait of a sheepadoodle wearing a cape",
      response_format: "b64_json",
      n: 1,
    }
  );

  console.log(image.data);
}

main();
Compreensão de áudio
Analisar a entrada de áudio:

Python
JavaScript
REST

import fs from "fs";
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "GEMINI_API_KEY",
  baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/",
});

const audioFile = fs.readFileSync("/path/to/your/audio/file.wav");
const base64Audio = Buffer.from(audioFile).toString("base64");

async function main() {
  const response = await client.chat.completions.create({
    model: "gemini-2.0-flash",
    messages: [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: "Transcribe this audio",
          },
          {
            type: "input_audio",
            input_audio: {
              data: base64Audio,
              format: "wav",
            },
          },
        ],
      },
    ],
  });

  console.log(response.choices[0].message.content);
}

main();
Saída estruturada
Os modelos do Gemini podem gerar objetos JSON em qualquer estrutura definida.

Python
JavaScript

import OpenAI from "openai";
import { zodResponseFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai"
});

const CalendarEvent = z.object({
  name: z.string(),
  date: z.string(),
  participants: z.array(z.string()),
});

const completion = await openai.beta.chat.completions.parse({
  model: "gemini-2.0-flash",
  messages: [
    { role: "system", content: "Extract the event information." },
    { role: "user", content: "John and Susan are going to an AI conference on Friday" },
  ],
  response_format: zodResponseFormat(CalendarEvent, "event"),
});

const event = completion.choices[0].message.parsed;
console.log(event);
Embeddings
Os embeddings de texto medem a relação entre strings de texto e podem ser gerados usando a API Gemini.

Python
JavaScript
REST

import OpenAI from "openai";

const openai = new OpenAI({
    apiKey: "GEMINI_API_KEY",
    baseURL: "https://generativelanguage.googleapis.com/v1beta/openai/"
});

async function main() {
  const embedding = await openai.embeddings.create({
    model: "text-embedding-004",
    input: "Your text string goes here",
  });

  console.log(embedding);
}

main();
Limitações atuais
O suporte para as bibliotecas do OpenAI ainda está na versão Beta enquanto ampliamos o suporte a recursos.

Se você tiver dúvidas sobre parâmetros compatíveis, recursos futuros ou problemas para começar a usar o Gemini, participe do nosso fórum para desenvolvedores.

####################################################################



Geração de texto

A API Gemini pode gerar saída de texto em resposta a várias entradas, incluindo texto, imagens, vídeo e áudio. Este guia mostra como gerar texto usando entradas de texto e imagem. Ele também abrange streaming, chat e instruções do sistema.

Antes de começar
Antes de chamar a API Gemini, verifique se você tem o SDK de sua escolha instalado e uma chave da API Gemini configurada e pronta para uso.

Entrada de texto
A maneira mais simples de gerar texto usando a API Gemini é fornecer ao modelo uma única entrada de texto, conforme mostrado neste exemplo:

Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "How does AI work?",
  });
  console.log(response.text);
}

await main();
Entrada de imagem
A API Gemini oferece suporte a entradas multimodais que combinam arquivos de texto e mídia. O exemplo a seguir mostra como gerar texto com base em texto e imagem:

Python
JavaScript
Go
REST
Apps Script

import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  const image = await ai.files.upload({
    file: "/path/to/organ.png",
  });
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: [
      createUserContent([
        "Tell me about this instrument",
        createPartFromUri(image.uri, image.mimeType),
      ]),
    ],
  });
  console.log(response.text);
}

await main();
Saída de streaming
Por padrão, o modelo retorna uma resposta após concluir todo o processo de geração de texto. Você pode ter interações mais rápidas usando o streaming para retornar instâncias de GenerateContentResponse conforme são geradas.

Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  const response = await ai.models.generateContentStream({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works",
  });

  for await (const chunk of response) {
    console.log(chunk.text);
  }
}

await main();
Conversas com vários turnos
O SDK do Gemini permite coletar várias rodadas de perguntas e respostas em uma conversa. O formato de chat permite que os usuários avancem gradualmente para encontrar respostas e receber ajuda com problemas de várias partes. Essa implementação do SDK do chat fornece uma interface para acompanhar o histórico de conversas, mas, nos bastidores, ela usa o mesmo método generateContent para criar a resposta.

O exemplo de código a seguir mostra uma implementação básica de chat:

Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.0-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const response1 = await chat.sendMessage({
    message: "I have 2 dogs in my house.",
  });
  console.log("Chat response 1:", response1.text);

  const response2 = await chat.sendMessage({
    message: "How many paws are in my house?",
  });
  console.log("Chat response 2:", response2.text);
}

await main();
Também é possível usar o streaming com chat, conforme mostrado no exemplo a seguir:

Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  const chat = ai.chats.create({
    model: "gemini-2.0-flash",
    history: [
      {
        role: "user",
        parts: [{ text: "Hello" }],
      },
      {
        role: "model",
        parts: [{ text: "Great to meet you. What would you like to know?" }],
      },
    ],
  });

  const stream1 = await chat.sendMessageStream({
    message: "I have 2 dogs in my house.",
  });
  for await (const chunk of stream1) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }

  const stream2 = await chat.sendMessageStream({
    message: "How many paws are in my house?",
  });
  for await (const chunk of stream2) {
    console.log(chunk.text);
    console.log("_".repeat(80));
  }
}

await main();
Parâmetros de configuração
Cada comando enviado ao modelo inclui parâmetros que controlam como o modelo gera respostas. É possível configurar esses parâmetros ou permitir que o modelo use as opções padrão.

O exemplo a seguir mostra como configurar os parâmetros do modelo:

Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works",
    config: {
      maxOutputTokens: 500,
      temperature: 0.1,
    },
  });
  console.log(response.text);
}

await main();
Confira alguns dos parâmetros de modelo que você pode configurar. As convenções de nomenclatura variam de acordo com a linguagem de programação.

stopSequences: especifica o conjunto de sequências de caracteres (até 5) que interromperá a geração de saída. Se especificado, a API vai parar na primeira aparição de um stop_sequence. A sequência de paradas não será incluída como parte da resposta.
temperature: controla a aleatoriedade da saída. Use valores mais altos para respostas mais criativas e valores mais baixos para respostas mais deterministas. Os valores podem variar de [0,0 a 2,0].
maxOutputTokens: define o número máximo de tokens a serem incluídos em um candidato.
topP: muda a forma como o modelo seleciona tokens para saída. Os tokens são selecionados do mais para o menos provável até que a soma das probabilidades seja igual ao valor topP. O valor padrão de topP é 0,95.
topK: muda a forma como o modelo seleciona tokens para saída. Um topK de 1 significa que o token selecionado é o mais provável entre todos os tokens no vocabulário do modelo, enquanto um topK de 3 significa que o próximo token é selecionado entre os três mais prováveis usando a temperatura. Os tokens são filtrados com base em topP, com o token final selecionado usando a amostragem de temperatura.
Instruções do sistema
As instruções do sistema permitem orientar o comportamento de um modelo com base no seu caso de uso específico. Ao fornecer instruções do sistema, você proporciona ao modelo mais contexto para ajudar a entender a tarefa e gerar respostas mais personalizadas. O modelo precisa aderir às instruções do sistema durante toda a interação com o usuário, permitindo que você especifique o comportamento no nível do produto separado dos comandos fornecidos pelos usuários finais.

É possível definir instruções do sistema ao inicializar o modelo:

Python
JavaScript
Go
REST
Apps Script

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Hello there",
    config: {
      systemInstruction: "You are a cat. Your name is Neko.",
    },
  });
  console.log(response.text);
}

await main();
Em seguida, é possível enviar solicitações ao modelo normalmente.

####################################################################



Compreensão de imagens

Os modelos Gemini podem processar imagens, permitindo muitos casos de uso de desenvolvedores de fronteira que, historicamente, exigiriam modelos específicos de domínio. Alguns dos recursos de visão do Gemini incluem a capacidade de:

Adicionar legendas e responder a perguntas sobre imagens
Transcrever e analisar PDFs, incluindo até 2 milhões de tokens
Detectar objetos em uma imagem e retornar as coordenadas da caixa delimitadora
Segmentar objetos em uma imagem
O Gemini foi criado para ser multimodal desde o início, e continuamos avançando os limites do que é possível. Este guia mostra como usar a API Gemini para gerar respostas de texto com base em entradas de imagem e realizar tarefas comuns de compreensão de imagem.

Antes de começar
Antes de chamar a API Gemini, verifique se você tem o SDK de sua escolha instalado e uma chave da API Gemini configurada e pronta para uso.

Entrada de imagem
Você pode fornecer imagens como entrada para o Gemini das seguintes maneiras:

Faça upload de um arquivo de imagem usando a API File antes de fazer uma solicitação para generateContent. Use esse método para arquivos maiores que 20 MB ou quando você quiser reutilizar o arquivo em várias solicitações.
Transmita dados de imagem inline com a solicitação para generateContent. Use esse método para arquivos menores (<20 MB de tamanho total da solicitação) ou imagens extraídas diretamente de URLs.
Fazer upload de um arquivo de imagem
É possível usar a API Files para fazer upload de um arquivo de imagem. Sempre use a API Files quando o tamanho total da solicitação (incluindo o arquivo, o comando de texto, as instruções do sistema etc.) for maior que 20 MB ou se você quiser usar a mesma imagem em vários comandos.

O código abaixo faz upload de um arquivo de imagem e o usa em uma chamada para generateContent.

Python
JavaScript
Go
REST

import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.jpg",
    config: { mimeType: "image/jpeg" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Caption this image.",
    ]),
  });
  console.log(response.text);
}

await main();
Para saber mais sobre como trabalhar com arquivos de mídia, consulte a API Files.

Transmitir dados de imagem inline
Em vez de fazer upload de um arquivo de imagem, é possível transmitir dados de imagem inline na solicitação para generateContent. Isso é adequado para imagens menores (tamanho de solicitação total de menos de 20 MB) ou imagens buscadas diretamente de URLs.

É possível fornecer dados de imagem como strings codificadas em Base64 ou lendo arquivos locais diretamente (dependendo do SDK).

Arquivo de imagem local:

Python
JavaScript
Go
REST

import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const base64ImageFile = fs.readFileSync("path/to/small-sample.jpg", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "image/jpeg",
      data: base64ImageFile,
    },
  },
  { text: "Caption this image." },
];

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: contents,
});
console.log(response.text);
Imagem do URL:

Python
JavaScript
Go
REST

import { GoogleGenAI } from "@google/genai";

async function main() {
  const ai = new GoogleGenAI({ apiKey: process.env.GOOGLE_API_KEY });

  const imageUrl = "https://goo.gle/instrument-img";

  const response = await fetch(imageUrl);
  const imageArrayBuffer = await response.arrayBuffer();
  const base64ImageData = Buffer.from(imageArrayBuffer).toString('base64');

  const result = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: [
    {
      inlineData: {
        mimeType: 'image/jpeg',
        data: base64ImageData,
      },
    },
    { text: "Caption this image." }
  ],
  });
  console.log(result.text);
}

main();
Alguns lembretes sobre os dados de imagem inline:

O tamanho máximo total da solicitação é de 20 MB, incluindo comandos de texto, instruções do sistema e todos os arquivos fornecidos inline. Se o tamanho do arquivo fizer com que o tamanho total da solicitação exceda 20 MB, use a API Files para fazer upload de um arquivo de imagem para usar na solicitação.
Se você estiver usando uma amostra de imagem várias vezes, será mais eficiente fazer upload de um arquivo de imagem usando a API File.
Solicitação com várias imagens
É possível fornecer várias imagens em um único comando incluindo vários objetos Part de imagem na matriz contents. Eles podem ser uma mistura de dados inline (arquivos locais ou URLs) e referências da API File.

Python
JavaScript
Go
REST

import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

async function main() {
  // Upload the first image
  const image1_path = "path/to/image1.jpg";
  const uploadedFile = await ai.files.upload({
    file: image1_path,
    config: { mimeType: "image/jpeg" },
  });

  // Prepare the second image as inline data
  const image2_path = "path/to/image2.png";
  const base64Image2File = fs.readFileSync(image2_path, {
    encoding: "base64",
  });

  // Create the prompt with text and multiple images
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      "What is different between these two images?",
      createPartFromUri(uploadedFile.uri, uploadedFile.mimeType),
      {
        inlineData: {
          mimeType: "image/png",
          data: base64Image2File,
        },
      },
    ]),
  });
  console.log(response.text);
}

await main();
Extrair uma caixa delimitadora para um objeto
Os modelos do Gemini são treinados para identificar objetos em uma imagem e fornecer as coordenadas da caixa delimitadora. As coordenadas são retornadas em relação às dimensões da imagem, dimensionadas para [0, 1000]. É necessário redimensionar essas coordenadas com base no tamanho da imagem original.

Python
JavaScript
Go
REST

const prompt = "Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.";
É possível usar caixas delimitadoras para a detecção e a localização de objetos em imagens e vídeos. Ao identificar e delimitar objetos com caixas limitadoras, você pode desbloquear uma ampla gama de aplicativos e melhorar a inteligência dos seus projetos.

Principais vantagens
Simples:integre recursos de detecção de objetos aos seus aplicativos com facilidade, independentemente da sua experiência em visão computacional.
Personalizável:produz caixas delimitadoras com base em instruções personalizadas (por exemplo, "Quero ver caixas delimitadoras de todos os objetos verdes nesta imagem") sem precisar treinar um modelo personalizado.
Detalhes técnicos
Entrada:seu comando e as imagens ou frames de vídeo associados.
Saída:caixas delimitadoras no formato [y_min, x_min, y_max, x_max]. O canto superior esquerdo é a origem. Os eixos x e y vão horizontalmente e verticalmente, respectivamente. Os valores de coordenadas são normalizados de 0 a 1.000 para cada imagem.
Visualização:os usuários do AI Studio vão ver as caixas de limite plotadas na interface.
Para desenvolvedores Python, teste o notebook de compreensão espacial 2D ou o notebook experimental de ponteiro 3D.

Normalizar coordenadas
O modelo retorna coordenadas da caixa delimitadora no formato [y_min, x_min, y_max, x_max]. Para converter essas coordenadas normalizadas nas coordenadas de pixel da imagem original, siga estas etapas:

Divida cada coordenada de saída por 1.000.
Multiplique as coordenadas x pela largura da imagem original.
Multiplique as coordenadas y pela altura da imagem original.
Para conferir exemplos mais detalhados de como gerar coordenadas de caixa delimitadora e visualizá-las em imagens, consulte o exemplo de livro de receitas de Detecção de objetos.

####################################################################




Compreensão de áudio

O Gemini pode analisar e entender a entrada de áudio, permitindo casos de uso como estes:

Descrever, resumir ou responder a perguntas sobre conteúdo de áudio.
Forneça uma transcrição do áudio.
Analisar segmentos específicos do áudio.
Este guia mostra como usar a API Gemini para gerar uma resposta de texto para entrada de áudio.

Antes de começar
Antes de chamar a API Gemini, verifique se você tem o SDK de sua escolha instalado e uma chave da API Gemini configurada e pronta para uso.

Áudio de entrada
Você pode fornecer dados de áudio para o Gemini das seguintes maneiras:

Faça upload de um arquivo de áudio antes de fazer uma solicitação para generateContent.
Transmita dados de áudio inline com a solicitação para generateContent.
Fazer upload de um arquivo de áudio
É possível usar a API Files para fazer upload de um arquivo de áudio. Sempre use a API Files quando o tamanho total da solicitação (incluindo arquivos, texto, instruções do sistema etc.) for maior que 20 MB.

O código a seguir faz upload de um arquivo de áudio e o usa em uma chamada para generateContent.

Python
JavaScript
Go
REST

import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp3",
    config: { mimeType: "audio/mp3" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Describe this audio clip",
    ]),
  });
  console.log(response.text);
}

await main();
Para saber mais sobre como trabalhar com arquivos de mídia, consulte a API Files.

Transmitir dados de áudio inline
Em vez de fazer upload de um arquivo de áudio, é possível transmitir dados de áudio inline na solicitação para generateContent:

Python
JavaScript
Go

import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const base64AudioFile = fs.readFileSync("path/to/small-sample.mp3", {
  encoding: "base64",
});

const contents = [
  { text: "Please summarize the audio." },
  {
    inlineData: {
      mimeType: "audio/mp3",
      data: base64AudioFile,
    },
  },
];

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: contents,
});
console.log(response.text);
Confira alguns lembretes sobre os dados de áudio inline:

O tamanho máximo da solicitação é de 20 MB, incluindo comandos de texto, instruções do sistema e arquivos fornecidos inline. Se o tamanho do arquivo fizer com que o tamanho total da solicitação exceda 20 MB, use a API Files para fazer upload de um arquivo de áudio para uso na solicitação.
Se você estiver usando um sample de áudio várias vezes, será mais eficiente fazer upload de um arquivo de áudio.
Acessar uma transcrição
Para receber uma transcrição dos dados de áudio, basta solicitar no prompt:

Python
JavaScript
Go

import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const result = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
    "Generate a transcript of the speech.",
  ]),
});
console.log("result.text=", result.text);
Consultar carimbos de data/hora
É possível se referir a seções específicas de um arquivo de áudio usando carimbos de data/hora do formulário MM:SS. Por exemplo, a seguinte solicitação pede uma transcrição que

Começa 2 minutos e 30 segundos após o início do arquivo.
Termina em 3 minutos e 29 segundos do início do arquivo.

Python
JavaScript
Go

// Create a prompt containing timestamps.
const prompt = "Provide a transcript of the speech from 02:30 to 03:29."
Contar Tokens
Chame o método countTokens para conferir o número de tokens em um arquivo de áudio. Exemplo:

Python
JavaScript
Go

import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const myfile = await ai.files.upload({
  file: "path/to/sample.mp3",
  config: { mimeType: "audio/mpeg" },
});

const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: createUserContent([
    createPartFromUri(myfile.uri, myfile.mimeType),
  ]),
});
console.log(countTokensResponse.totalTokens);
Formatos de áudio compatíveis
O Gemini oferece suporte aos seguintes tipos MIME de formato de áudio:

WAV - audio/wav
MP3 - audio/mp3
AIFF - audio/aiff
AAC - audio/aac
OGG Vorbis - audio/ogg
FLAC - audio/flac
Detalhes técnicos sobre áudio
O Gemini representa cada segundo de áudio como 32 tokens. Por exemplo, um minuto de áudio é representado como 1.920 tokens.
O Gemini só pode inferir respostas a falas em inglês.
O Gemini pode "entender" componentes não de fala, como o canto de pássaros ou sirenes.
A duração máxima de dados de áudio em uma única instrução é de 9,5 horas. O Gemini não limita o número de arquivos de áudio em uma única instrução.No entanto, a duração total combinada de todos os arquivos de áudio em uma única instrução não pode exceder 9,5 horas.
O Gemini reduz os arquivos de áudio para uma resolução de dados de 16 Kbps.
Se a fonte de áudio tiver vários canais, o Gemini vai combiná-los em um único canal.

####################################################################



Embasamento com a Pesquisa Google

Python JavaScript REST

O recurso "Embasadas na Pesquisa Google" na API Gemini e no AI Studio pode ser usado para melhorar a precisão e a recência das respostas do modelo. Além de respostas mais factuais, quando o Embasamento com a Pesquisa Google está ativado, a API Gemini retorna fontes de fundamentação (links de suporte inline) e sugestões da Pesquisa Google junto com o conteúdo da resposta. As Sugestões de pesquisa direcionam os usuários aos resultados de pesquisa correspondentes à resposta fundamentada.

Este guia vai ajudar você a começar a usar o Embasamento com a Pesquisa Google.

Antes de começar
Antes de chamar a API Gemini, verifique se você tem o SDK de sua escolha instalado e uma chave da API Gemini configurada e pronta para uso.

Configurar o embasamento da pesquisa
A partir do Gemini 2.0, a Pesquisa Google está disponível como ferramenta. Isso significa que o modelo pode decidir quando usar a Pesquisa Google. O exemplo a seguir mostra como configurar a Pesquisa como uma ferramenta.


import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: [
        "Who individually won the most bronze medals during the Paris olympics in 2024?",
    ]
    config: {
      tools: [{googleSearch: {}}],
    },
  });
  console.log(response.text);
  // To get grounding metadata as web content.
  console.log(response.candidates[0].groundingMetadata.searchEntryPoint.renderedContent)
}

await main();
A funcionalidade de pesquisa como ferramenta também permite pesquisas em vários turnos e consultas com várias ferramentas. A combinação da pesquisa com a chamada de função ainda não é aceita.

A pesquisa como ferramenta permite comandos e fluxos de trabalho complexos que exigem planejamento, raciocínio e pensamento:

Embasamento para melhorar a precisão e a atualidade e fornecer respostas mais precisas
Recuperar artefatos da Web para fazer outras análises em
Encontrar imagens, vídeos ou outras mídias relevantes para ajudar em tarefas de raciocínio ou geração multimodais
Programação, solução de problemas técnicos e outras tarefas especializadas
Encontrar informações específicas da região ou ajudar a traduzir conteúdo com precisão
Como encontrar sites relevantes para navegar
O Embasamento com a Pesquisa Google funciona com todos os idiomas disponíveis ao fazer comandos de texto. No nível pago da API Gemini Developer, você pode receber 1.500 consultas de Grounding com a Pesquisa Google por dia sem custos financeiros, com consultas adicionais cobradas no padrão de US $35 por 1.000 consultas.

Sugestões de pesquisa do Google
Para usar o Embasamento com a Pesquisa Google, você precisa mostrar as Sugestões de pesquisa do Google, que são consultas sugeridas incluídas nos metadados da resposta embasada. Para saber mais sobre os requisitos de exibição, consulte Usar as Sugestões da Pesquisa Google.

Recuperação da Pesquisa Google
Observação: a recuperação da Pesquisa Google é compatível apenas com os modelos Gemini 1.5. Para modelos do Gemini 2.0, use a Pesquisa como uma ferramenta.
O exemplo a seguir mostra como configurar um modelo para usar a recuperação da Pesquisa Google (ele só funciona com modelos 1.5):


import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  let response = await ai.models.generateContent({
    model: "gemini-1.5-flash",
    contents: [
        "Who individually won the most silver medals during the Paris olympics in 2024?",
    ],
    config: {
      tools: [{googleSearchRetrieval:{}}],
    },
  })

  console.log(response.text)
}
As configurações mode e dynamicThreshold permitem controlar o comportamento de recuperação dinâmica, proporcionando controle adicional sobre quando para usar o Grounding com a Pesquisa Google.


import {GoogleGenAI, DynamicRetrievalConfigMode} from '@google/genai';

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  let response = await ai.models.generateContent({
    model: "gemini-1.5-flash",
    contents: [
        "Who individually won the most gold medals during the Paris olympics in 2024?",
    ],
    config: {
      tools: [{
        googleSearchRetrieval:{
          dynamicRetrievalConfig:{
            dynamicThreshold:0.5,
            mode:DynamicRetrievalConfigMode.MODE_DYNAMIC
          }
        }
      }],
    },
  })

  console.log(response.text)
}
Recuperação dinâmica
Observação: a recuperação dinâmica é compatível apenas com o Flash 1.5 do Gemini. Para o Gemini 2.0, use a Pesquisa como uma ferramenta, conforme mostrado acima.
Algumas consultas vão aproveitar o embasamento na Pesquisa Google mais do que outras. O recurso de recuperação dinâmica oferece mais controle sobre quando usar o Embasamento com a Pesquisa Google.

Se o modo de recuperação dinâmica não for especificado, a ativação da Pesquisa Google será sempre acionada. Se o modo estiver definido como dinâmico, o modelo vai decidir quando usar a aterramento com base em um limite que você pode configurar. O limite é um valor de ponto flutuante no intervalo [0,1] e tem o valor padrão 0,3. Se o valor de limite for 0, a resposta sempre será com embasamento na Pesquisa Google. Se for 1, ela nunca será.

Como a recuperação dinâmica funciona
Você pode usar a recuperação dinâmica na sua solicitação para escolher quando ativar o embasamento com a Pesquisa Google. Isso é útil quando o comando não exige uma resposta com embasamento na Pesquisa Google e o modelo pode fornecer uma resposta com base no próprio conhecimento sem embasamento. Isso ajuda a gerenciar a latência, a qualidade e o custo com mais eficiência.

Antes de invocar a configuração de recuperação dinâmica na solicitação, entenda a seguinte terminologia:

Pontuação da previsão: quando você solicita uma resposta com embasamento, o Gemini atribui uma pontuação de previsão ao comando. A pontuação de previsão é um valor de ponto flutuante no intervalo [0,1]. O valor depende se a instrução pode se beneficiar ao basear a resposta com as informações mais atualizadas da Pesquisa Google. Portanto, se uma instrução exigir uma resposta com base nos fatos mais recentes da Web, ela terá uma pontuação de previsão mais alta. Um comando para o qual uma resposta gerada pelo modelo é suficiente tem uma pontuação de previsão mais baixa.

Confira exemplos de algumas solicitações e as notas de previsão delas.

Observação: as pontuações de previsão são atribuídas pelo Gemini e podem variar ao longo do tempo, dependendo de vários fatores.
Comando	Pontuação de previsão	Comentário
"Escreva um poema sobre peônias"	0,13	O modelo pode confiar no próprio conhecimento, e a resposta não precisa de embasamento.
"Sugira um brinquedo para uma criança de 2 anos"	0.36	O modelo pode confiar no próprio conhecimento, e a resposta não precisa de embasamento.
"Você pode dar uma receita de guacamole de inspiração asiática?"	0,55	A Pesquisa Google pode dar uma resposta com embasamento, mas o embasamento não é estritamente necessário. O conhecimento do modelo pode ser suficiente.
"O que é o Agent Builder? Como o embasamento é cobrado no Agent Builder?"	0,72	Requer que a Pesquisa Google gere uma resposta com bom embasamento.
"Quem ganhou o último Grande Prêmio de F1?"	0.97	Requer que a Pesquisa Google gere uma resposta com bom embasamento.
Limite: na solicitação de API, é possível especificar uma configuração de recuperação dinâmica com um limite. O limite é um valor de ponto flutuante no intervalo [0,1] e tem o valor padrão 0,3. Se o valor de limite for zero, a resposta será sempre com embasamento com a Pesquisa Google. Para todos os outros valores de limite, o seguinte é aplicável:

Se a pontuação da previsão for maior ou igual ao limite, a resposta será baseada na Pesquisa Google. Um limite mais baixo implica que mais comandos têm respostas geradas usando o Embasamento com a Pesquisa Google.
Se a pontuação da previsão for menor que o limite, o modelo ainda poderá gerar a resposta, mas ela não será baseada na Pesquisa Google.
Para saber como definir o limite de recuperação dinâmica usando um SDK ou a API REST, consulte o exemplo de código apropriado.

Para encontrar um bom limite que atenda às necessidades da sua empresa, crie um conjunto representativo de consultas que você espera encontrar. Em seguida, você pode classificar as consultas de acordo com a pontuação de previsão na resposta e selecionar um limite adequado para seu caso de uso.

Uma resposta fundamentada
Se o comando for embasado corretamente na Pesquisa Google, a resposta vai incluir groundingMetadata. Uma resposta com base pode ser semelhante a esta (partes da resposta foram omitidas para encurtar):


{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships. He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall. \n"
          }
        ],
        "role": "model"
      },
      ...
      "groundingMetadata": {
        "searchEntryPoint": {
          "renderedContent": "\u003cstyle\u003e\n.container {\n  align-items: center;\n  border-radius: 8px;\n  display: flex;\n  font-family: Google Sans, Roboto, sans-serif;\n  font-size: 14px;\n  line-height: 20px;\n  padding: 8px 12px;\n}\n.chip {\n  display: inline-block;\n  border: solid 1px;\n  border-radius: 16px;\n  min-width: 14px;\n  padding: 5px 16px;\n  text-align: center;\n  user-select: none;\n  margin: 0 8px;\n  -webkit-tap-highlight-color: transparent;\n}\n.carousel {\n  overflow: auto;\n  scrollbar-width: none;\n  white-space: nowrap;\n  margin-right: -12px;\n}\n.headline {\n  display: flex;\n  margin-right: 4px;\n}\n.gradient-container {\n  position: relative;\n}\n.gradient {\n  position: absolute;\n  transform: translate(3px, -9px);\n  height: 36px;\n  width: 9px;\n}\n@media (prefers-color-scheme: light) {\n  .container {\n    background-color: #fafafa;\n    box-shadow: 0 0 0 1px #0000000f;\n  }\n  .headline-label {\n    color: #1f1f1f;\n  }\n  .chip {\n    background-color: #ffffff;\n    border-color: #d2d2d2;\n    color: #5e5e5e;\n    text-decoration: none;\n  }\n  .chip:hover {\n    background-color: #f2f2f2;\n  }\n  .chip:focus {\n    background-color: #f2f2f2;\n  }\n  .chip:active {\n    background-color: #d8d8d8;\n    border-color: #b6b6b6;\n  }\n  .logo-dark {\n    display: none;\n  }\n  .gradient {\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n  }\n}\n@media (prefers-color-scheme: dark) {\n  .container {\n    background-color: #1f1f1f;\n    box-shadow: 0 0 0 1px #ffffff26;\n  }\n  .headline-label {\n    color: #fff;\n  }\n  .chip {\n    background-color: #2c2c2c;\n    border-color: #3c4043;\n    color: #fff;\n    text-decoration: none;\n  }\n  .chip:hover {\n    background-color: #353536;\n  }\n  .chip:focus {\n    background-color: #353536;\n  }\n  .chip:active {\n    background-color: #464849;\n    border-color: #53575b;\n  }\n  .logo-light {\n    display: none;\n  }\n  .gradient {\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n  }\n}\n\u003c/style\u003e\n\u003cdiv class=\"container\"\u003e\n  \u003cdiv class=\"headline\"\u003e\n    \u003csvg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\n      \u003cpath fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/\u003e\n      \u003cpath fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/\u003e\n      \u003cpath fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/\u003e\n      \u003cpath fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/\u003e\n    \u003c/svg\u003e\n    \u003csvg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\"\u003e\n      \u003ccircle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/\u003e\n      \u003cpath d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/\u003e\n      \u003cpath d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/\u003e\n      \u003cpath d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/\u003e\n      \u003cpath d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/\u003e\n    \u003c/svg\u003e\n    \u003cdiv class=\"gradient-container\"\u003e\u003cdiv class=\"gradient\"\u003e\u003c/div\u003e\u003c/div\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"carousel\"\u003e\n    \u003ca class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4x8Epe-gzpwRBvp7o3RZh2m1ygq1EHktn0OWCtvTXjad4bb1zSuqfJd6OEuZZ9_SXZ_P2SvCpJM7NaFfQfiZs6064MeqXego0vSbV9LlAZoxTdbxWK1hFeqTG6kA13YJf7Fbu1SqBYM0cFM4zo0G_sD9NKYWcOCQMvDLDEJFhjrC9DM_QobBIAMq-gWN95G5tvt6_z6EuPN8QY=\"\u003ewho won wimbledon 2024\u003c/a\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n"
        },
        "groundingChunks": [
          {
            "web": {
              "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4whET1ta3sDETZvcicd8FeNe4z0VuduVsxrT677KQRp2rYghXI0VpfYbIMVI3THcTuMwggRCbFXS_wVvW0UmGzMe9h2fyrkvsnQPJyikJasNIbjJLPX0StM4Bd694-ZVle56MmRA4YiUvwSqad1w6O2opmWnw==",
              "title": "wikipedia.org"
            }
          },
          {
            "web": {
              "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wR1M-9-yMPUr_KdHlnoAmQ8ZX90DtQ_vDYTjtP2oR5RH4tRP04uqKPLmesvo64BBkPeYLC2EpVDxv9ngO3S1fs2xh-e78fY4m0GAtgNlahUkm_tBm_sih5kFPc7ill9u2uwesNGUkwrQlmP2mfWNU5lMMr23HGktr6t0sV0QYlzQq7odVoBxYWlQ_sqWFH",
              "title": "wikipedia.org"
            }
          },
          {
            "web": {
              "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wsDmROzbP-tmt8GdwCW_pqISTZ4IRbBuoaMyaHfcQg8WW-yKRQQvMDTPAuLxJh-8_U8_iw_6JKFbQ8M9oVYtaFdWFK4gOtL4RrC9Jyqc5BNpuxp6uLEKgL5-9TggtNvO97PyCfziDFXPsxylwI1HcfQdrz3Jy7ZdOL4XM-S5rC0lF2S3VWW0IEAEtS7WX861meBYVjIuuF_mIr3spYPqWLhbAY2Spj-4_ba8DjRvmevIFUhRuESTKvBfmpxNSM",
              "title": "cbssports.com"
            }
          },
          {
            "web": {
              "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4yzjLkorHiUKjhOPkWaZ9b4cO-cLG-02vlEl6xTBjMUjyhK04qSIclAa7heR41JQ6AAVXmNdS3WDrLOV4Wli-iezyzW8QPQ4vgnmO_egdsuxhcGk3-Fp8-yfqNLvgXFwY5mPo6QRhvplOFv0_x9mAcka18QuAXtj0SPvJfZhUEgYLCtCrucDS5XFc5HmRBcG1tqFdKSE1ihnp8KLdaWMhrUQI21hHS9",
              "title": "jagranjosh.com"
            }
          },
          {
            "web": {
              "uri": "https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4y9L4oeNGWCatFz63b9PpP3ys-Wi_zwnkUT5ji9lY7gPUJQcsmmE87q88GSdZqzcx5nZG9usot5FYk2yK-FAGvCRE6JsUQJB_W11_kJU2HVV1BTPiZ4SAgm8XDFIxpCZXnXmEx5HUfRqQm_zav7CvS2qjA2x3__qLME6Jy7R5oza1C5_aqjQu422le9CaigThS5bvJoMo-ZGcXdBUCj2CqoXNVjMA==",
              "title": "apnews.com"
            }
          }
        ],
        "groundingSupports": [
          {
            "segment": {
              "endIndex": 85,
              "text": "Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships."
            },
            "groundingChunkIndices": [
              0,
              1,
              2,
              3
            ],
            "confidenceScores": [
              0.97380733,
              0.97380733,
              0.97380733,
              0.97380733
            ]
          },
          {
            "segment": {
              "startIndex": 86,
              "endIndex": 210,
              "text": "He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall."
            },
            "groundingChunkIndices": [
              1,
              0,
              4
            ],
            "confidenceScores": [
              0.96145374,
              0.96145374,
              0.96145374
            ]
          }
        ],
        "webSearchQueries": [
          "who won wimbledon 2024"
        ]
      }
    }
  ],
  ...
}
Se a resposta não incluir groundingMetadata, isso significa que a resposta não foi fundamentada. Isso pode acontecer por vários motivos, incluindo baixa relevância da fonte ou informações incompletas na resposta do modelo.

Quando um resultado com base é gerado, os metadados contêm URIs que redirecionam para os editores do conteúdo usado para gerar o resultado com base. Esses URIs contêm o subdomínio vertexaisearch, como neste exemplo truncado: https://vertexaisearch.cloud.google.com/grounding-api-redirect/.... Os metadados também contêm os domínios dos editores. Os URIs fornecidos permanecem acessíveis por 30 dias após a geração do resultado com embasamento.

Importante: os URIs fornecidos precisam ser acessíveis diretamente pelos usuários finais e não podem ser consultados de forma programática por meios automatizados. Se o acesso automatizado for detectado, o serviço de geração de respostas com embasamento poderá parar de fornecer os URIs de redirecionamento.
O campo renderedContent em searchEntryPoint é o código fornecido para implementar as sugestões da Pesquisa Google. Consulte Usar as sugestões da Pesquisa Google para saber mais.   

####################################################################        


Compreensão do vídeo

Os modelos Gemini podem processar vídeos, permitindo muitos casos de uso de desenvolvedores de fronteira que, historicamente, exigiriam modelos específicos de domínio. Alguns dos recursos de visão do Gemini incluem a capacidade de:

Descrever, segmentar e extrair informações de vídeos com até 90 minutos de duração
Responder a perguntas sobre o conteúdo do vídeo
Fazer referência a marcações de tempo específicas em um vídeo
O Gemini foi criado para ser multimodal desde o início, e continuamos avançando os limites do que é possível. Este guia mostra como usar a API Gemini para gerar respostas de texto com base em entradas de vídeo.

Antes de começar
Antes de chamar a API Gemini, verifique se você tem o SDK de sua escolha instalado e uma chave da API Gemini configurada e pronta para uso.

Entrada de vídeo
Você pode fornecer vídeos como entrada para o Gemini das seguintes maneiras:

Envie um arquivo de vídeo usando a API File antes de fazer uma solicitação para generateContent. Use esse método para arquivos maiores que 20 MB, vídeos com mais de um minuto ou quando você quiser reutilizar o arquivo em várias solicitações.
Transmita dados de vídeo inline com a solicitação para generateContent. Use esse método para arquivos menores (menos de 20 MB) e durações mais curtas.
Inclua um URL do YouTube diretamente no comando.
Fazer upload de um arquivo de vídeo
É possível usar a API Files para fazer upload de um arquivo de vídeo. Sempre use a API Files quando o tamanho total da solicitação (incluindo o arquivo, o comando de texto, as instruções do sistema etc.) for maior que 20 MB, a duração do vídeo for significativa ou se você pretende usar o mesmo vídeo em vários comandos.

A API File aceita formatos de arquivo de vídeo diretamente. Este exemplo usa o filme curto da NASA "Jupiter's Great Red Spot Shrinks and Grows". Crédito: Centro de Voos Espaciais Goddard (GSFC)/David Ladd (2018).

"Jupiter's Great Red Spot Shrinks and Grows" está no domínio público e não mostra pessoas identificáveis. (Diretrizes de uso de imagens e mídia da NASA.)

O código a seguir faz o download do vídeo de exemplo, faz o upload dele usando a API File, espera que ele seja processado e, em seguida, usa a referência do arquivo em uma solicitação generateContent.

Python
JavaScript
Go
REST

import {
  GoogleGenAI,
  createUserContent,
  createPartFromUri,
} from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });

async function main() {
  const myfile = await ai.files.upload({
    file: "path/to/sample.mp4",
    config: { mimeType: "video/mp4" },
  });

  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: createUserContent([
      createPartFromUri(myfile.uri, myfile.mimeType),
      "Summarize this video. Then create a quiz with an answer key based on the information in this video.",
    ]),
  });
  console.log(response.text);
}

await main();
Para saber mais sobre como trabalhar com arquivos de mídia, consulte a API Files.

Transmitir dados de vídeo inline
Em vez de fazer upload de um arquivo de vídeo usando a API File, é possível transmitir vídeos menores diretamente na solicitação para generateContent. Isso é adequado para vídeos mais curtos com menos de 20 MB de tamanho de solicitação.

Confira um exemplo de como fornecer dados de vídeo inline:

Python
JavaScript
REST

import { GoogleGenAI } from "@google/genai";
import * as fs from "node:fs";

const ai = new GoogleGenAI({ apiKey: "GOOGLE_API_KEY" });
const base64VideoFile = fs.readFileSync("path/to/small-sample.mp4", {
  encoding: "base64",
});

const contents = [
  {
    inlineData: {
      mimeType: "video/mp4",
      data: base64VideoFile,
    },
  },
  { text: "Please summarize the video in 3 sentences." }
];

const response = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: contents,
});
console.log(response.text);
Incluir um URL do YouTube
Prévia :o recurso de URL do YouTube está em fase de testes e disponível sem custo financeiro. Os preços e os limites de taxa podem mudar.
A API Gemini e o AI Studio oferecem suporte a URLs do YouTube como Part de dados de arquivo. É possível incluir um URL do YouTube com um comando que pede ao modelo para resumir, traduzir ou interagir com o conteúdo do vídeo.

Limitações:

Não é possível enviar mais de oito horas de vídeo do YouTube por dia.
Você só pode enviar um vídeo por solicitação.
Só é possível enviar vídeos públicos, não privados ou não listados.
O exemplo a seguir mostra como incluir um URL do YouTube com uma solicitação:

Python
JavaScript
Go
REST

import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-1.5-pro" });
const result = await model.generateContent([
  "Please summarize the video in 3 sentences.",
  {
    fileData: {
      fileUri: "https://www.youtube.com/watch?v=9hE5-98ZeCg",
    },
  },
]);
console.log(result.response.text());
Consulte as marcações de tempo no conteúdo
Você pode fazer perguntas sobre pontos específicos no vídeo usando carimbos de data/hora do formulário MM:SS.

Python
JavaScript
Go
REST

const prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?";
Transcrever vídeos e fornecer descrições visuais
Os modelos Gemini podem transcrever e fornecer descrições visuais do conteúdo de vídeo processando a faixa de áudio e os frames visuais. Para descrições visuais, o modelo faz a amostragem do vídeo a uma taxa de 1 frame por segundo. Essa taxa de amostragem pode afetar o nível de detalhes nas descrições, principalmente em vídeos com mudanças visuais rápidas.

Python
JavaScript
Go
REST

const prompt = "Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions.";
Formatos de vídeo compatíveis:
O Gemini oferece suporte aos seguintes tipos MIME de formato de vídeo:

video/mp4
video/mpeg
video/mov
video/avi
video/x-flv
video/mpg
video/webm
video/wmv
video/3gpp
Detalhes técnicos sobre vídeos
Modelos e contexto com suporte: todos os modelos Gemini 2.0 e 2.5 podem processar dados de vídeo.
Os modelos com uma janela de contexto de 2 milhões podem processar vídeos de até 2 horas, enquanto os modelos com uma janela de contexto de 1 milhão podem processar vídeos de até 1 hora.
Processamento da API File: ao usar a API File, os vídeos são amostrados a 1 frame por segundo (QPS) e o áudio é processado a 1 Kbps (canal único). Os carimbos de data/hora são adicionados a cada segundo.
Essas taxas estão sujeitas a mudanças no futuro para melhorias na inferência.
Cálculo de tokens: cada segundo de vídeo é tokenizado da seguinte maneira:
Frames individuais (amostrados a 1 QPS): 258 tokens por frame.
Áudio: 32 tokens por segundo.
Os metadados também são incluídos.
Total: aproximadamente 300 tokens por segundo de vídeo.
Formato de carimbo de data/hora: ao se referir a momentos específicos em um vídeo no comando, use o formato MM:SS (por exemplo, 01:15 por 1 minuto e 15 segundos).
Práticas recomendadas:
Use apenas um vídeo por comando para ter os melhores resultados.
Se você combinar texto e um único vídeo, coloque o comando de texto após a parte de vídeo na matriz contents.
Sequências de ação rápidas podem perder detalhes devido à taxa de amostragem de 1 QPS. Diminua a velocidade desses clipes, se necessário.

####################################################################



